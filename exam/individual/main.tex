\documentclass[format=acmsmall, review=false, screen=true]{acmart}
\settopmatter{printacmref=false} % Removes citation information below abstract
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
\pagestyle{plain} % removes running headers
\acmYear{2017}
\acmMonth{12}

\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{wrapfig}

\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
  frame=single,
  breaklines=true,
  postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
  aboveskip=10pt,
  belowskip=5pt,
  tabsize=2
} 

\setlength{\textfloatsep}{14pt}
\setlength{\abovecaptionskip}{4pt}
\setlength{\belowcaptionskip}{4pt}

\author{Richard BÃ¡nyi}

\title{\textsc{Exam Assignment} - Big Data Management (Technical) }
\acmDOI{}
\begin{document}
\maketitle 

\section{Question 1 - Creepiness and Ethics }


\section{Question 2 - Systems and Data Models }

\textit{Alluxio} is a disparate in-memory storage system that provides high performance and reliability. It enables to manage and transform large datasets at memory speed. \textit{Alluxio} can be deployed as a middleware between Storage (HDFS) and Batch (computation) layer in the \textit{lambda architecture}. The new representation of the \textit{lambda architecture} with \textit{Alluxio} is shown in figure.

To be able to compare our system architecture during projects and the architecture using \textit{Alluxio} we have to understand the different data processing approaches.

Computation and data can be either co-located in the same cluster or can be decoupled. In our projects, we were given the first approach were computation and data is co-located, this approach avoids expensive data transfers and network I/O utilization. As \textit{Alluxio} tutorial\footnote{\url{https://www.alluxio.com/blog/getting-started-with-alluxio-and-spark}} states this scenario does not provide much performance benefits of \textit{Alluxio}. Therofe I believe using \textit{Alluxio} in our projects would not lead to any performance improvments.

However in real production environments computation and data are more likely separated. The data is distributed over multiple data centers, therefore accessing data requires to transfer data from the remote data center to the computation data center, which introduces large network latency. Also HDFS uses spinning disks, therefore all the I/O operations, especially write, have high latency. This is the time when \textit{Alluxios} in-memory storage system comes in hand, which stores the frequently used data and allows memory speed acces. The following figure depicts the data operations with \textit{Alluxio}.

As an example from \textit{Alluxio}\footnote{\url{http://alluxio-com-site-prod.s3.amazonaws.com/resource/media/Baidu-Case-Study.pdf}} if our environment was based on real production, \textit{Alluxio} system could be implemented the follwing way:

\begin{enumerate}
	\item A query or a batch process is submitted. The task is analyzed and the manager looks up if the data is already in \textit{Alluxio}.
	\item If the data is already in \textit{Alluxio}, the task manager grabs the data from \textit{Alluxio} and performs the analysis according to the submitted task.  
	\item If data is not in \textit{Alluxio}, it's called a cache miss, and the manager request the data directly from the remote HDFS. Meanwile, another job is submitted by \textit{Alluxio} manager which requests the data from the remote HDFS and stores the data in \textit{Alluxio}.
\end{enumerate}
 
 In our case, we could have the raw data stored in \textit{Alluxio} and immediatelly avaiblable at every iteration of the batch process. Moreover, after cleaning and normalization of the data we could store our views in memory, so that our presentation layer could start serve immediatelly and run many queries and much lower lattency. In contrast, in our Project 2, in order to avoid disk read/write operations, we used Spark to process the master dataset and temporarily cached the results in memory as DataFrame. But this approach is cumbersome, each time we modified our implementation we had to restart the Spark context which dropped the cached data and had to reaload the entire master dataset, which resulted in more downtime.   

Benefits of using \textit{Alluxio} in the \textit{lambda architecture}
\begin{itemize}
		\item 
\end{itemize}

\section{Question 3 - Data Cleaning and Storage }

\subsection{A}

The actual process of data cleaning differed in both projects. In project 2 we have used the Spark SQL interface to clean the data and in project 3 we have moved to the Spark functional interface, therefore our methods for cleaning where not generalized, thus we could not reuse any cleaning process from project 2. Also both projects required a different validation constraints.

However, a common data cleaning process in both cases was consolidating the empty or unknown values. We captured null, empty, or invalid values and we have assigned a custom value to repsesent as a missing piece of data.


\subsection{B}
As a rule of thumb when designing a Big Data system, we want to ensure that we will be able to answer as many question as possible from the given data. Thefore storing the raw/unclean data is a better option. Mainly because, storing the rawest data allows to to maximize the ability to gain new insights that were not considered when exemining the dataset. On the other hand storing the raw data entails more storage requirements and more resources but generaly speaking in the big data ecosystem with technologies like \textit{Hadoop} it should be able to manage large amounts of data in a distributed, scalable manner. Also it is better to store the unclean data to master dataset, because the data cleaning process may improve or the requirements may broaden, therefore the data is subject to change over time.

For example in project 3, we came accross to a form of unstructured data \textit{lane/edge} field, and our semantic normalization process could derive 4 other different information, if later on this normalization would improve we could perhaps derive knowledge more accurately and used that information for another purpose. By storing the cleaned data we would limit what the data can tell. 

\section{Question 4 - Views and Applications }

\subsection{A}

\subsection{B}

A possible \textit{Lambda Architecture} approach that faciliates real-time data processing (streaming) is depicted in the following figure:

\begin{wrapfigure}{r}{0.7\textwidth}[H]
\begin{center}
  \includegraphics[width=0.46\textwidth]{lambda_architecture.jpg}
  \caption{A possible \textit{Lambda Architecture} approach that faciliates real-time data processing (streaming)}
  \label{fig:lambda-architecture}
\end{center}
\end{wrapfigure}

\newpage

All new data (in XML format) is sent to both the batch layer and to the speed layer. In the batch layer, new data is appended to the master data set. The data is immutable and append-only stored on the storage layer. The batch layer precomputes the views continuously.

\textbf{Storage Layer} as a storage system we could use \textit{HDFS}. However, in a cloud environment, many organizations do not operate \textit{HDFS} but use instead \textit{Amazon S3} as the storage backend. 

\textbf{Batch Layer} is achieved by \textit{Apache Spark} which's main function is to pre-compute the batch views.

\textbf{Serving Layer} indexes the batch views and provides interface to the pre-computed views so that can be queried. For this purposes \textit{Hive} is used to store the pre-computed batch views.

\textbf{Speed Layer} in order to compensate the high latency of updates to the serving layer, \textit{Spark Streaming} is used to produce the realtime views. The realtime views are updated as data comes in, thus reusing the previous realtime views. The real-time views are always up-to-date and are stored in databases which requires random-read/random-writes.

Queries are resolved by merging the batch and real-time views, which can be accomplised also by the \textit{Spark Framework}. Both batch layer and speed layer produces a table in the serving database. An user application can issue a query, which merges those results. The following figure summarize the idea:

\begin{figure}[H]
  \includegraphics[width=0.56\linewidth]{speed_layer.jpg}
  \caption{Resolving queries by mergin \textit{batch tables} and \textit{speed tables}.}
  \label{fig:speed-layer}
\end{figure}

The following figure shows a possible implementation of the given problem in real life. 

\begin{figure}[H]
  \includegraphics[width=0.66\linewidth]{implementation.jpg}
  \caption{High-level overview of the architecture}
  \label{fig:speed-layer}
\end{figure}

Real-time data flows to the system through \textit{Kafka} (service that can store and process stream of data). It then gets aggregted and stored in HDFS. Thus, all the historical data resides in the master dataset.

\textit{Apache Spark} handles the batch computation. The pre-computed views are than stored in \textit{Apache Hive} with proper indexing. Due to the latency of the batch layer, the results from the serving layer are always out-of-date by few hours/minutes. Therefore this implementation includes a speed layer with \textit{Spark Streaming}, which digest the data from \textit{Kafka} and process the data. The processed real-time views are than pushed out to the \textit{Apache Cassandra} database. And lastly, the query layer is responsible for merging the batch and real-time views using the \textit{Spark API}. It allows to express SQL queries programmatically using the \textit{Spark SQL Interface}, also using more general purposes languages like \textit{Python, Scala, or Java}. Thus using complex analytical algorithms. 
\end{document}
